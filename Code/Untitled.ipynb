{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM for MLE and MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Target Distribution\n",
    "Recall that in our model, we suppose that our data, $\\mathbf{X}=\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}$ is drawn from the mixture of $K$ number of Gaussian distributions. For each observation $\\mathbf{x}_n$ we have a latent variable $\\mathbf{z}_n$ that is a 1-of-$K$ binary vector with elements $z_{nk}$. We denote the set of latent variable by $\\mathbf{Z}$. Recall that the distibution of $\\mathbf{Z}$ given the mixing coefficients, $\\pi$, is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{Z} | \\pi) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \n",
    "\\end{align}\n",
    "Recall also that the likelihood of the data is given by,\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) =\\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}\\left(\\mathbf{x}_n| \\mu_k, \\Sigma_k\\right)^{z_{nk}}\n",
    "\\end{align}\n",
    "Finally, in our basic model, we choose a Dirichlet prior for $\\pi$ \n",
    "\\begin{align}\n",
    "p(\\pi) = \\mathrm{Dir}(\\pi | \\alpha_0) = C(\\alpha_0) \\prod_{k=1}^K \\pi_k^{\\alpha_0 -1},\n",
    "\\end{align}\n",
    "where $C(\\alpha_0)$ is the normalizing constant for the Dirichlet distribution. We also choose a Normal-Inverse-Wishart prior for the mean and the covariance of the likelihood function\n",
    "\\begin{align}\n",
    "p(\\mu, \\Sigma) = p(\\mu | \\Sigma) p(\\Sigma) = \\prod_{k=1}^K \\mathcal{N}\\left(\\mu_k | \\mathbf{m}_0, \\mathbf{V}_0\\right) IW(\\Sigma_k|\\mathbf{S}_0, \\nu_0).\n",
    "\\end{align}\n",
    "Thus, the joint distribution of all the random variable is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Sigma) = p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) p(\\mathbf{Z} | \\pi) p(\\pi) p(\\mu | \\Sigma) p(\\Sigma)\n",
    "\\end{align}\n",
    "\n",
    "### EM for MLE\n",
    "\n",
    "#### E-step:\n",
    "*Needs exposition*\n",
    "\\begin{align}\n",
    "r_{nk} = \\frac{\\pi_k p\\left(\\mathbf{x}_n | \\mu_k, \\Sigma_k\\right)}{\\sum_{k'=1}^K \\pi_{k'} p\\left(\\mathbf{x}_n | \\mu_{k'}, \\Sigma_{k'}\\right)}\n",
    "\\end{align}\n",
    "\n",
    "#### M-step:\n",
    "*Needs exposition*\n",
    "\\begin{align}\n",
    "\\pi_k &= \\frac{r_k}{N},\\;\\; r_k =\\sum_{n=1}^N r_{nk}\\\\\n",
    "\\mu_k &=\\frac{\\sum_{n=1}^N r_{nk}\\mathbf{x}_n}{r_k}\\\\\n",
    "\\Sigma_k &= \\frac{\\sum_{n=1}^N r_{nk} \\mathbf{x}_n\\mathbf{x}_n^\\top}{r_k} - \\mu_k\\mu_k^\\top\n",
    "\\end{align}\n",
    "\n",
    "### EM for MAP\n",
    "\n",
    "#### E-step:\n",
    "*Needs exposition*\n",
    "\\begin{align}\n",
    "r_{nk} = \\frac{\\pi_k p\\left(\\mathbf{x}_n | \\mu_k, \\Sigma_k\\right)}{\\sum_{k'=1}^K \\pi_{k'} p\\left(\\mathbf{x}_n | \\mu_{k'}, \\Sigma_{k'}\\right)}\n",
    "\\end{align}\n",
    "\n",
    "#### M-step:\n",
    "*Needs exposition*\n",
    "\\begin{align}\n",
    "\\pi_k &= \\frac{r_k + \\alpha_k - 1}{N + \\sum_{k=1}^K \\alpha_k - K},\\;\\; r_k =\\sum_{n=1}^N r_{nk}\\\\\n",
    "\\hat{\\mu}_k &=\\frac{r_k \\overline{\\mathbf{x}}_k + \\beta_0 \\mathrm{m}_0}{r_k + \\beta_0}\\\\\n",
    "\\overline{\\mathbf{x}}_k&= \\frac{\\sum_{n=1}^N r_{nk} \\mathbf{x}_n}{r_k}\\\\\n",
    "\\hat{\\Sigma}_k &= \\frac{\\mathbf{S}_0 + \\mathbf{S}_k + \\frac{\\beta_0r_k}{\\beta_0 + r_k}(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)^\\top}{\\nu_0 + r_k + D + 2}\\\\\n",
    "\\mathbf{S}_k &= \\sum_{n=1}^N r_{nk} (\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)(\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)^\\top\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEKCAYAAADdBdT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1079bb190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "from sklearn import mixture\n",
    "from scipy.stats import multivariate_normal as MVN\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#-------------   data parameters\n",
    "N = 100 #number of observations\n",
    "K = 2 #number of components\n",
    "M = 2 #number of features per observation\n",
    "D = 2 #degree of freedom\n",
    "\n",
    "#-------------   some toy data\n",
    "\n",
    "n_samples = int(N/2.0) #number of points in each component\n",
    "mu1 = np.array([10, 10]) #mean of component 1\n",
    "mu2 = np.array([6, 6]) #mean of component 1\n",
    "\n",
    "# generate random sample, two components\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate spherical data centered on mu1\n",
    "comp1 = np.random.randn(n_samples, M) + mu1\n",
    "\n",
    "# generate spherical data centered on mu2\n",
    "comp2 = np.random.randn(N - n_samples, M) + mu2\n",
    "\n",
    "# concatenate the two datasets into training set\n",
    "data = np.vstack([comp1, comp2])\n",
    "\n",
    "#plot the components\n",
    "plt.scatter(comp1[:, 0], comp1[:, 1], 4, color='r')\n",
    "plt.scatter(comp2[:, 0], comp2[:, 1], 4, color='b')\n",
    "\n",
    "plt.title('test data')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.60589567  9.64535927]\n",
      " [ 5.82280779  6.10284942]]\n"
     ]
    }
   ],
   "source": [
    "#we should initialize with k-means pi, mu, Sigma\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "#####         EM for MLE           #####\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "def MLE_EM(K, D, N, X, pi_0, mu_0, Sigma_0, iters):\n",
    "    \n",
    "    r = np.zeros((N, K))\n",
    "    pi = pi_0\n",
    "    mu = mu_0\n",
    "    Sigma = Sigma_0\n",
    "    \n",
    "    #---------------- Likelihood ----------------#    \n",
    "    def lkhd(pi, mu, Sigma):\n",
    "        prob = np.zeros((N, K))\n",
    "        for k in xrange(K):\n",
    "            prob[:, k] = pi[k] * MVN.pdf(X, mu[k, :], Sigma[k])\n",
    "        prob[ prob < 1e-50] = 0\n",
    "        return prob\n",
    "    \n",
    "    #---------------- E-Step ----------------#\n",
    "    def E_step():\n",
    "        prob = lkhd(pi, mu, Sigma) \n",
    "        return np.diag(np.reciprocal(np.sum(prob, axis=1))).dot(prob)\n",
    "        \n",
    "    \n",
    "    #---------------- M-Step ----------------#\n",
    "    def M_step():\n",
    "        r_ks = np.sum(r, axis=0)\n",
    "        pi_new = 1. / N * r_ks\n",
    "        mu_new = np.diag(np.reciprocal(r_ks)).dot(r.T.dot(X))\n",
    "        \n",
    "        Sigma_new = []\n",
    "        for k in xrange(K):\n",
    "            Sigma_k = np.zeros((D, D))\n",
    "            for n in xrange(N):\n",
    "                Sigma_k += r[n, k] * np.outer(X[n, :] - mu[k, :], X[n, :] - mu[k, :])\n",
    "            Sigma_new.append(Sigma_k / r_ks[k])\n",
    "            \n",
    "        return pi_new, mu_new, Sigma_new\n",
    "    \n",
    "    #---------------- Alternate Between E and M-steps ----------------#\n",
    "    for i in xrange(iters):        \n",
    "        r = E_step()\n",
    "        pi, mu, Sigma = M_step()\n",
    "        \n",
    "    r = E_step()\n",
    "    \n",
    "    return pi, mu, Sigma, r\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "#####         EM for MAP           #####\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "def MAP_EM(K, D, N, X, pi_0, mu_0, Sigma_0, S_0, m_0, nu_0, beta_0, alpha_0, iters):\n",
    "    \n",
    "    #initialization of intermediate parameters\n",
    "    r = np.zeros((N, K))\n",
    "    pi = pi_0\n",
    "    mu = mu_0\n",
    "    Sigma = Sigma_0\n",
    "    S = [np.eye(D) for k in xrange(K)]\n",
    "    X_mean = mu_0\n",
    "    \n",
    "    #---------------- Likelihood ----------------#    \n",
    "    def lkhd(pi, mu, Sigma):\n",
    "        prob = np.zeros((N, K))\n",
    "        for k in xrange(K):\n",
    "            prob[:, k] = pi[k] * MVN.pdf(X, mu[k, :], Sigma[k])        \n",
    "        prob[ prob < 1e-50] = 0\n",
    "        return prob\n",
    "    \n",
    "    #---------------- E-Step ----------------#\n",
    "    def E_step():\n",
    "        prob = lkhd(pi, mu, Sigma) \n",
    "        return np.diag(np.reciprocal(np.sum(prob, axis=1))).dot(prob)\n",
    "        \n",
    "    \n",
    "    #---------------- M-Step ----------------#\n",
    "    def M_step():\n",
    "        r_ks = np.sum(r, axis=0)\n",
    "        pi_new = (r_ks + alpha_0 - 1) * 1. / (N + np.sum(alpha_0) - K)\n",
    "        X_mean_new = np.nan_to_num(np.diag(np.reciprocal(r_ks)).dot(r.T.dot(X)))\n",
    "        mu_new = np.nan_to_num(np.diag(np.reciprocal(r_ks \n",
    "                                                     + beta_0)).dot(np.diag(r_ks).dot(X_mean) \n",
    "                                                                    + beta_0 * m_0))\n",
    "        S_new = []\n",
    "        Sigma_new = []\n",
    "        for k in xrange(K):            \n",
    "            c_1 = (beta_0 * r_ks[k]) / (beta_0 * r_ks[k])\n",
    "            c_2 = nu_0 + r_ks[k] + D + 2\n",
    "            Sigma_k = np.nan_to_num(S_0 + S[k] + c_1 \n",
    "                                    * np.outer(X_mean[k, :] - m_0, X_mean[k, :] - m_0))\n",
    "            Sigma_new.append(Sigma_k * 1./c_2)\n",
    "            \n",
    "            S_k = np.zeros((D, D))\n",
    "            for n in xrange(N):\n",
    "                S_k += r[n, k] * np.outer(X[n, :] - X_mean[k, :], X[n, :] - X_mean[k, :])\n",
    "                \n",
    "            S_new.append(S_k)\n",
    "        return pi_new, X_mean_new, mu_new, S_new, Sigma_new\n",
    "        \n",
    "    #---------------- Alternate Between E and M-steps ----------------#\n",
    "    for i in xrange(iters): \n",
    "        r = np.nan_to_num(E_step())\n",
    "        pi, X_mean, mu, S, Sigma = M_step()\n",
    "        \n",
    "    r = E_step()\n",
    "    \n",
    "    return pi, X_mean, mu, S, Sigma, r\n",
    "\n",
    "iters = 100\n",
    "\n",
    "X = np.array([[10, 10], [21, 21], [8, 8.5], [19, 19]])\n",
    "mu = np.array([[9, 9], [5, 5]])\n",
    "Sigma = [1. * np.eye(D), 1. * np.eye(D)]\n",
    "pi = np.array([0.2, 0.8])\n",
    "\n",
    "pi, mu, Sigma, r = MLE_EM(K, D, N, data, pi, mu, Sigma, iters)\n",
    "print mu\n",
    "\n",
    "\n",
    "#random initialization of hyperparameters\n",
    "alpha_0 = np.random.random(K)\n",
    "beta_0 = np.random.random()\n",
    "m_0 = np.zeros(D)\n",
    "S_0 = np.eye(D)\n",
    "nu_0 = D + 1\n",
    "\n",
    "pi, X_mean, mu, S, Sigma, r = MAP_EM(K, D, N, data, pi, mu, Sigma, S_0, m_0, nu_0, beta_0, alpha_0, iters)\n",
    "\n",
    "print mu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
