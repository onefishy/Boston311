{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampler for Bayesian GMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Target Distribution\n",
    "Recall that in our model, we suppose that our data, $\\mathbf{X}=\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$ is drawn from the mixture of $K$ number of Gaussian distributions. For each observation $\\mathbf{x}_n$ we have a latent variable $\\mathbf{z}_n$ that is a 1-of-$K$ binary vector with elements $z_{nk}$. We denote the set of latent variable by $\\mathbf{Z}$. Recall that the distibution of $\\mathbf{Z}$ given the mixing coefficients, $\\pi$, is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{Z} | \\pi) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \n",
    "\\end{align}\n",
    "Recall also that the likelihood of the data is given by,\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) =\\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}\\left(\\mathbf{x}_n| \\mu_k, \\Sigma_k\\right)^{z_{nk}}\n",
    "\\end{align}\n",
    "Finally, in our basic model, we choose a Dirichlet prior for $\\pi$ \n",
    "\\begin{align}\n",
    "p(\\pi) = \\mathrm{Dir}(\\pi | \\alpha_0) = C(\\alpha_0) \\prod_{k=1}^K \\pi_k^{\\alpha_0 -1},\n",
    "\\end{align}\n",
    "where $C(\\alpha_0)$ is the normalizing constant for the Dirichlet distribution. We also choose a Normal-Inverse-Wishart prior for the mean and the covariance of the likelihood function\n",
    "\\begin{align}\n",
    "p(\\mu, \\Sigma) = p(\\mu | \\Sigma) p(\\Sigma) = \\prod_{k=1}^K \\mathcal{N}\\left(\\mu_k | \\mathbf{m}_0, \\mathbf{V}_0\\right) IW(\\Sigma_k|\\mathbf{S}_0, \\nu_0).\n",
    "\\end{align}\n",
    "Thus, the joint distribution of all the random variable is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{Z}, \\pi, \\mu, \\Sigma | \\mathbf{X}) = p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) p(\\mathbf{Z} | \\pi) p(\\pi) p(\\mu | \\Sigma) p(\\Sigma)\n",
    "\\end{align}\n",
    "\n",
    "### Gibbs Samper\n",
    "The full conditionals are as follows:\n",
    "1. $p(\\mathbf{z}_{n} = \\delta(k) | \\mathbf{x}_n, \\mu, \\Sigma, \\pi)  \\propto \\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)$.\n",
    "2. $p(\\pi|\\mathbf{Z}) = \\mathrm{Dir}(\\{ \\alpha_k + \\sum_{i=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))\\}_{k=1}^K)$\n",
    "3. $p(\\mu_k | \\Sigma_k, \\mathbf{Z}, \\mathbf{X}) = \\mathcal{N}(\\mu_k | m_k, V_k)$\n",
    "4. $\\mathbf{V}_k^{-1} = \\mathbf{V}_0^{-1} + N_k\\Sigma_k^{-1}$\n",
    "5. $\\mathbf{m}_k = \\mathbf{V}_k(\\Sigma_k^{-1}N_k\\overline{\\mathbf{x}}_k + \\mathbf{V}_0^{-1}\\mathbf{m}_0)$\n",
    "6. $N_k = \\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))$\n",
    "7. $\\overline{\\mathbf{x}}_k = \\displaystyle \\frac{\\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))\\mathbf{x}_n}{N_k}$\n",
    "8. $p(\\Sigma_k | \\mu_k, \\mathbf{z}, \\mathbf{x}) = IW(\\Sigma_k | \\mathbf{S}_k, \\nu_k)$\n",
    "9. $\\mathbf{S}_k = \\mathbf{S}_0 + \\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))(\\mathbf{x}_n - \\mu_k)(\\mathbf{x}_n - \\mu_k)^\\top$\n",
    "10. $\\nu_k = \\nu_0 + N_k$\n",
    "\n",
    "The algorithm for the sampler is as follows:\n",
    "1. Instantiate the latent variables randomly.\n",
    "2. For $k=1...K$:\n",
    "    3. For $n=1...N$: update $z_i$ by sampling from $p(\\mathbf{z}_{n} = \\delta(k) | \\mathbf{x}_n, \\mu, \\Sigma, \\pi)$.\n",
    "    4. Update $\\pi$\n",
    "    5. Update variables for each component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68276335,  0.04153166,  0.82237552],\n",
       "       [ 0.81003542,  0.67225182,  0.03770043],\n",
       "       [ 0.29673968,  0.53954806,  0.56597002],\n",
       "       [ 0.81821523,  0.9556991 ,  0.83974211],\n",
       "       [ 0.64051746,  0.1884108 ,  0.69722839],\n",
       "       [ 0.46902524,  0.39094551,  0.85628296],\n",
       "       [ 0.44434655,  0.02226147,  0.41945852],\n",
       "       [ 0.3543623 ,  0.93396813,  0.14651943],\n",
       "       [ 0.82850378,  0.84815628,  0.59560089],\n",
       "       [ 0.85452344,  0.67944386,  0.56578022]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "K = 3\n",
    "dimensions = 4\n",
    "mu = np.random.uniform(0,1,[N,K])\n",
    "# Sigma = np.random.uniform(0,1,[N,K])\n",
    "# X = np.random.uniform(0,1,[N,dimensions])\n",
    "# scipy.stats.norm(mu, Sigma).pdf(X)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.19903245,  5.27221668,  5.5466585 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(mu, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It would be easier to debug if we implement each update separately\n",
    "def update_Z(delta_k, X, mu, Sigma, pi):\n",
    "    Z = pi * scipy.stats.norm(mu, Sigma).pdf(X)\n",
    "    \n",
    "    return Z\n",
    "\n",
    "def update_pi(alpha_k, Z):\n",
    "    pi = np.random.dirichlet()m\n",
    "    return pi\n",
    "\n",
    "def update_mu_k(m_k, V_k):\n",
    "    return mu_k\n",
    "\n",
    "def update_V_k(N_k, Sigma_k):\n",
    "    return V_k\n",
    "\n",
    "def update_m_k(V_k, Sigma_k, N_k, mean_x_k, V_0, m_0):\n",
    "    return m_k\n",
    "\n",
    "def update_N_k(Z):\n",
    "    return N_k\n",
    "\n",
    "def update_mean_x_k(Z, X, N_k):\n",
    "    return mean_x_k\n",
    "\n",
    "def update_Sigma_k(S_k, nu_k):\n",
    "    return Sigma_k\n",
    "\n",
    "def update_S_k(S_0, Z, X, mu_k):\n",
    "    return S_k\n",
    "\n",
    "def update_nu_k(nu_0, N_k):\n",
    "    return nu_k\n",
    "\n",
    "# this function can be reused even when we change the prior on pi \n",
    "def gibbs_gmm(K, X, nu_0, S_0, V_0, m_0, alpha_0):\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3],[4,5,6]]\n",
    "np.tile(a,(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
