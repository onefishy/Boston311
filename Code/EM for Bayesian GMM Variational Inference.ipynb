{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM for Bayesian GMM Variational Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Target Distribution\n",
    "Recall that in our model, we suppose that our data, $\\mathbf{X}=\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}$ is drawn from the mixture of $K$ number of Gaussian distributions. For each observation $\\mathbf{x}_n$ we have a latent variable $\\mathbf{z}_n$ that is a 1-of-$K$ binary vector with elements $z_{nk}$. We denote the set of latent variable by $\\mathbf{Z}$. Recall that the distibution of $\\mathbf{Z}$ given the mixing coefficients, $\\pi$, is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{Z} | \\pi) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \n",
    "\\end{align}\n",
    "Recall also that the likelihood of the data is given by,\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Lambda) =\\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}\\left(\\mathbf{x}_n| \\mu_k \\Lambda^{-1}_k\\right)^{z_{nk}}\n",
    "\\end{align}\n",
    "Finally, in our basic model, we choose a Dirichlet prior for $\\pi$ \n",
    "\\begin{align}\n",
    "p(\\pi) = \\mathrm{Dir}(\\pi | \\alpha_0) = C(\\alpha_0) \\prod_{k=1}^K \\pi_k^{\\alpha_0 -1},\n",
    "\\end{align}\n",
    "where $C(\\alpha_0)$ is the normalizing constant for the Dirichlet distribution. We also choose a Normal-Wishart prior for the mean and the precision of the likelihood function\n",
    "\\begin{align}\n",
    "p(\\mu, \\Lambda) = p(\\mu | \\Lambda) p(\\Lambda) = \\prod_{k=1}^K \\mathcal{N}\\left(\\mu_k | \\mathbf{m}_0, (\\beta_0\\Lambda_k)^{-1}\\right) \\mathcal{W}(\\Lambda_k|\\mathbf{W}_0, \\nu_0).\n",
    "\\end{align}\n",
    "Thus, the joint distribution of all the random variable is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Lambda) = p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Lambda) p(\\mathbf{Z} | \\pi) p(\\pi) p(\\mu | \\Lambda) p(\\Lambda)\n",
    "\\end{align}\n",
    "\n",
    "### Variational Approximation\n",
    "We consider a variational distribution which factorizes the latent variables and the parameters\n",
    "\\begin{align}\n",
    "q(\\mathbf{Z}, \\pi, \\mu, \\Lambda) = q(\\mathbf{Z})q(\\pi, \\mu, \\Lambda).\n",
    "\\end{align}\n",
    "Recall that we can decompose the log marginal probability as follows\n",
    "\\begin{align}\n",
    "\\ln p(\\mathbf{X}) = \\mathcal{L}(q) + \\mathrm{KL}(q\\| p),\n",
    "\\end{align}\n",
    "where $\\mathrm{KL}(q\\| p)$ is the Kullbackâ€“Leibler divergence of $q$ and $p$ and\n",
    "\\begin{align}\n",
    "\\mathcal{L}(q) = \\int q(\\mathbf{Z}) \\ln\\left \\{\\frac{p(\\mathbf{X}, \\mathbf{Z})}{p(\\mathbf{Z})} \\right\\}d \\mathbf{Z}.\n",
    "\\end{align}\n",
    "When $q(\\mathbf{Z})$ is exactly equal to the posterior distribution $p(\\mathbf{Z} | \\mathbf{X})$ we have that $\\mathrm{KL}(q\\| p) = 0$. Thus, fitting our approximate distribution $q$ to the target distribution $p$ is a matter of minimizing $\\mathrm{KL}(q\\| p)$. For our  factorized distribution, $q$, minimizing $\\mathrm{KL}(q\\| p)$ is equivalent to maximizing $\\mathcal{L}(q)$ (see Chapter 10 in Bishop for details). Finally, the solution of both optimization problem, $q^*$, occurs when\n",
    "\\begin{align}\n",
    "\\ln q^*(\\mathbf{Z}) &= \\mathbb{E}_{\\pi, \\mu, \\Lambda} \\left[\\ln p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Lambda) \\right] + \\text{const}\\\\\n",
    "\\ln q^*(\\pi, \\mu, \\Lambda) &= \\mathbb{E}_{\\mathbf{Z}} \\left[\\ln p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Lambda) \\right] + \\text{const}\n",
    "\\end{align}\n",
    "We will alternatingly re-estimate the parameters of an approximation $q^(i)$ of $q^*$ according to the following update rules. \n",
    "\n",
    "### Variational E-step\n",
    "At the $i$-th step, we have that\n",
    "\\begin{align}\n",
    "\\ln q^*(\\pi) = \\mathrm{Dir}(\\pi | \\alpha),\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\alpha &= (\\alpha_k)_{k=1}^K,\\;\\; \\alpha_k = \\alpha_0 + N_k\\\\\n",
    "N_k &= \\sum_{n=1}^N r_{n, k}\n",
    "\\end{align}\n",
    "We also have \n",
    "\\begin{align}\n",
    "q^*(\\mu_k, \\Lambda_k) = \\mathcal{N}(\\mu_k | \\mathbf{m}_k, (\\beta_k\\Lambda_k)^{-1})\\mathcal{W}(\\Lambda_k | \\mathbf{W}_k, \\mu_k)\n",
    "\\end{align}\n",
    "where \n",
    "\\begin{align}\n",
    "\\mathbf{m}_k &= \\frac{1}{\\beta_k} (\\beta_0\\mathbf{m}_0 + N_k\\overline{\\mathbf{x}}_k)\\\\\n",
    "\\beta_k &= \\beta_0 + N_k\\\\\n",
    "\\mathbf{W}_k^{-1} &=\\mathbf{W}_0^{-1} + N_k \\mathbf{S}_k + \\frac{\\beta_0N_k}{\\beta_0+N_k}(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)^\\top\\\\\n",
    "\\nu_k &= \\nu_0 + N_k + 1\\\\\n",
    "\\overline{\\mathbf{x}}_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{nk}\\mathbf{x}_n\\\\\n",
    "\\mathrm{S}_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} (\\mathbf{x}_k - \\overline{\\mathbf{x}}_k)(\\mathbf{x}_k - \\overline{\\mathbf{x}}_k)^\\top\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### Variational M-step\n",
    "At the $i$-step, we have that\n",
    "\\begin{align}\n",
    "q^*(\\mathbf{Z}) = \\prod_{n=1}^N\\prod_{k=1}^K r_{nk}^{z_{nk}},\n",
    "\\end{align}\n",
    "where \n",
    "\\begin{align}\n",
    "r_{nk} &= \\frac{\\rho_{nk}}{\\sum_{j=1}^K \\rho_{nj}}\\\\\n",
    "\\rho_{nk} &= \\exp\\left\\{-\\frac{1}{2} \\left[D\\beta_k^{-1} + \\nu_k(\\mathbf{x}_n - \\mathbf{m}_k)^\\top \\mathbf{W}_k (\\mathbf{x}_n - \\mathbf{m}_k) \\right] \\right\\}\n",
    "\\end{align}\n",
    "where $D$ is the dimension of each vector $\\mathbf{x}_n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
