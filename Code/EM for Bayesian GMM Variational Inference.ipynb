{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM for Bayesian GMM Variational Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Target Distribution\n",
    "Recall that in our model, we suppose that our data, $\\mathbf{X}=\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}$ is drawn from the mixture of $K$ number of Gaussian distributions. For each observation $\\mathbf{x}_n$ we have a latent variable $\\mathbf{z}_n$ that is a 1-of-$K$ binary vector with elements $z_{nk}$. We denote the set of latent variable by $\\mathbf{Z}$. Recall that the distibution of $\\mathbf{Z}$ given the mixing coefficients, $\\pi$, is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{Z} | \\pi) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \n",
    "\\end{align}\n",
    "Recall also that the likelihood of the data is given by,\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Lambda) =\\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}\\left(\\mathbf{x}_n| \\mu_k, \\Lambda^{-1}_k\\right)^{z_{nk}}\n",
    "\\end{align}\n",
    "Finally, in our basic model, we choose a Dirichlet prior for $\\pi$ \n",
    "\\begin{align}\n",
    "p(\\pi) = \\mathrm{Dir}(\\pi | \\alpha_0) = C(\\alpha_0) \\prod_{k=1}^K \\pi_k^{\\alpha_0 -1},\n",
    "\\end{align}\n",
    "where $C(\\alpha_0)$ is the normalizing constant for the Dirichlet distribution. We also choose a Normal-Wishart prior for the mean and the precision of the likelihood function\n",
    "\\begin{align}\n",
    "p(\\mu, \\Lambda) = p(\\mu | \\Lambda) p(\\Lambda) = \\prod_{k=1}^K \\mathcal{N}\\left(\\mu_k | \\mathbf{m}_0, (\\beta_0\\Lambda_k)^{-1}\\right) \\mathcal{W}(\\Lambda_k|\\mathbf{W}_0, \\nu_0).\n",
    "\\end{align}\n",
    "Thus, the joint distribution of all the random variable is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Lambda) = p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Lambda) p(\\mathbf{Z} | \\pi) p(\\pi) p(\\mu | \\Lambda) p(\\Lambda)\n",
    "\\end{align}\n",
    "\n",
    "### Variational Approximation\n",
    "We consider a variational distribution which factorizes the latent variables and the parameters\n",
    "\\begin{align}\n",
    "q(\\mathbf{Z}, \\pi, \\mu, \\Lambda) = q(\\mathbf{Z})q(\\pi, \\mu, \\Lambda).\n",
    "\\end{align}\n",
    "Recall that we can decompose the log marginal probability as follows\n",
    "\\begin{align}\n",
    "\\ln p(\\mathbf{X}) = \\mathcal{L}(q) + \\mathrm{KL}(q\\| p),\n",
    "\\end{align}\n",
    "where $\\mathrm{KL}(q\\| p)$ is the Kullbackâ€“Leibler divergence of $q$ and $p$ and\n",
    "\\begin{align}\n",
    "\\mathcal{L}(q) = \\int q(\\mathbf{Z}) \\ln\\left \\{\\frac{p(\\mathbf{X}, \\mathbf{Z})}{p(\\mathbf{Z})} \\right\\}d \\mathbf{Z}.\n",
    "\\end{align}\n",
    "When $q(\\mathbf{Z})$ is exactly equal to the posterior distribution $p(\\mathbf{Z} | \\mathbf{X})$ we have that $\\mathrm{KL}(q\\| p) = 0$. Thus, fitting our approximate distribution $q$ to the target distribution $p$ is a matter of minimizing $\\mathrm{KL}(q\\| p)$. For our  factorized distribution, $q$, minimizing $\\mathrm{KL}(q\\| p)$ is equivalent to maximizing $\\mathcal{L}(q)$ (see Chapter 10 in Bishop for details). Finally, the solution of both optimization problem, $q^*$, occurs when\n",
    "\\begin{align}\n",
    "\\ln q^*(\\mathbf{Z}) &= \\mathbb{E}_{\\pi, \\mu, \\Lambda} \\left[\\ln p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Lambda) \\right] + \\text{const}\\\\\n",
    "\\ln q^*(\\pi, \\mu, \\Lambda) &= \\mathbb{E}_{\\mathbf{Z}} \\left[\\ln p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Lambda) \\right] + \\text{const}\n",
    "\\end{align}\n",
    "We will alternatingly re-estimate the parameters of an approximation $q^{(i)}$ of $q^*$ according to the following update rules. \n",
    "\n",
    "\n",
    "### Variational E-step\n",
    "At the $i$-step, we have that\n",
    "\\begin{align}\n",
    "q^*(\\mathbf{Z}) = \\prod_{n=1}^N\\prod_{k=1}^K r_{nk}^{z_{nk}},\n",
    "\\end{align}\n",
    "where \n",
    "\\begin{align}\n",
    "r_{nk} &= \\frac{\\rho_{nk}}{\\sum_{j=1}^K \\rho_{nj}}\\\\\n",
    "\\rho_{nk} &= \\tilde{\\pi}_k\\tilde{\\Lambda}^{1/2}_k\\exp\\left\\{-\\frac{1}{2} \\left[D\\beta_k^{-1} + \\nu_k(\\mathbf{x}_n - \\mathbf{m}_k)^\\top \\mathbf{W}_k (\\mathbf{x}_n - \\mathbf{m}_k) \\right] \\right\\}\n",
    "\\end{align}\n",
    "where $D$ is the dimension of each vector $\\mathbf{x}_n$.\n",
    "\n",
    "\n",
    "\n",
    "### Variational M-step\n",
    "At the $i$-th step, we have that\n",
    "\\begin{align}\n",
    "q^*(\\pi) = \\mathrm{Dir}(\\pi | \\alpha),\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\alpha &= (\\alpha_k)_{k=1}^K,\\;\\; \\alpha_k = \\alpha_0 + N_k\\\\\n",
    "N_k &= \\sum_{n=1}^N r_{n, k}\n",
    "\\end{align}\n",
    "We also have \n",
    "\\begin{align}\n",
    "q^*(\\mu_k, \\Lambda_k) = \\mathcal{N}(\\mu_k | \\mathbf{m}_k, (\\beta_k\\Lambda_k)^{-1})\\mathcal{W}(\\Lambda_k | \\mathbf{W}_k, \\mu_k)\n",
    "\\end{align}\n",
    "where \n",
    "\\begin{align}\n",
    "\\mathbf{m}_k &= \\frac{1}{\\beta_k} (\\beta_0\\mathbf{m}_0 + N_k\\overline{\\mathbf{x}}_k)\\\\\n",
    "\\beta_k &= \\beta_0 + N_k\\\\\n",
    "\\mathbf{W}_k^{-1} &=\\mathbf{W}_0^{-1} + N_k \\mathbf{S}_k + \\frac{\\beta_0N_k}{\\beta_0+N_k}(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)^\\top\\\\\n",
    "\\nu_k &= \\nu_0 + N_k + 1\\\\\n",
    "\\overline{\\mathbf{x}}_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{nk}\\mathbf{x}_n\\\\\n",
    "\\mathbf{S}_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} (\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)(\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)^\\top\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "from sklearn import mixture\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEKCAYAAADdBdT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102a234d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-------------   data parameters\n",
    "N = 100 #number of observations\n",
    "K = 2 #number of components\n",
    "M = 2 #number of features per observation\n",
    "D = 2 #degree of freedom\n",
    "\n",
    "n_samples = int(N/2.0) #number of points in each component\n",
    "mu1 = np.array([10, 10]) #mean of component 1\n",
    "mu2 = np.array([6, 6]) #mean of component 1\n",
    "\n",
    "# generate random sample, two components\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate spherical data centered on mu1\n",
    "comp1 = np.random.randn(n_samples, M) + mu1\n",
    "\n",
    "# generate spherical data centered on mu2\n",
    "comp2 = np.random.randn(N - n_samples, M) + mu2\n",
    "\n",
    "# concatenate the two datasets into training set\n",
    "data = np.vstack([comp1, comp2])\n",
    "\n",
    "#plot the components\n",
    "plt.scatter(comp1[:, 0], comp1[:, 1], 4, color='r')\n",
    "plt.scatter(comp2[:, 0], comp2[:, 1], 4, color='b')\n",
    "\n",
    "plt.title('test data')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "#use a max log likelihood initializattion\n",
    "def variational_approx(X, K, D, N, pi, Lambda, mu):\n",
    "    #random initialization of hyperparameters\n",
    "    alpha_0 = np.random.random()\n",
    "    beta_0 = np.random.random()\n",
    "    m_0 = np.zeros(D)\n",
    "    W_0 = np.eye(D)\n",
    "    nu_0 = D + 1\n",
    "    \n",
    "    #intermediate variables\n",
    "    r = np.zeros((N, K))\n",
    "    rho = np.zeros((N, K))\n",
    "    Z = np.zeros((N, K))\n",
    "    S = []\n",
    "    x_mean = np.zeros((K, D))\n",
    "    nu = np.zeros(K)\n",
    "    W_k = np.zeros((D, D))\n",
    "    W = []\n",
    "    beta = np.zeros(K)\n",
    "    m = np.zeros((K, D))\n",
    "    alpha = np.zeros(K) \n",
    "    N_count = np.zeros(N)\n",
    "    \n",
    "    #---------------- Variational E-Step ----------------#\n",
    "    def E_step():\n",
    "        ####### parameters for q(Z)\n",
    "        #update rho\n",
    "        for k in xrange(K):\n",
    "            s_k = (X - m[k, :])\n",
    "            rho[:, k] = np.exp(-0.5 * (D * 1. / beta[k] + nu[k] * (s_k.T).dot(W_k.dot(s_k))))\n",
    "            \n",
    "        #update r    \n",
    "        rho_sum = np.sum(rho, axis=0)\n",
    "        for k in xrange(K):\n",
    "            r[:, k] = rho[:, k] * np.reciprocal(rho_sum)\n",
    "    \n",
    "    #---------------- Variational M-Step ----------------#\n",
    "    def M_step():\n",
    "        #######  parameters for q(pi)\n",
    "        #update N_count\n",
    "        N_count = np.sum(r, axis=1)  \n",
    "        \n",
    "        #update alpha\n",
    "        alpha += N_count\n",
    "        \n",
    "        #######  parameters for q(mu, Lambda)\n",
    "        #update beta\n",
    "        beta = beta_0 + N_count   \n",
    "        \n",
    "        #update nu\n",
    "        nu = nu_0 + N_count + 1\n",
    "        \n",
    "        #update x_mean\n",
    "        x_mean = np.reciprocal(N_count) * (r.T).dot(X)\n",
    "        \n",
    "        #update S_k\n",
    "        S = []\n",
    "        for k in xrange(K):\n",
    "            S_k = np.zeros((D, D))\n",
    "            for n in xrange(N):\n",
    "                S_k += (1. / N_count[k] \n",
    "                        * r[k, n] \n",
    "                        * np.outer(X[n, :] - x_mean[k], X[n, :] - x_mean[k]))\n",
    "            \n",
    "        #update m\n",
    "        m = np.reciprocal(beta) * (beta_0 * m_0 + np.diag(N_count).dot(x_mean))\n",
    "        \n",
    "        #update W_k\n",
    "        W = [np.linalg.inv(np.linalg.inv(W_0) + N_count[k] * S[k] \n",
    "                           + (beta_0 * N_count[k]) / (beta_0 + N_count[k]) \n",
    "                           * np.outer(x_mean[k, :] - m_0, x_mean[k, :] - m_0))\n",
    "             for k in xrange(K)] \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
