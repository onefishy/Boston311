{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampler for Bayesian GMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Target Distribution\n",
    "\n",
    "Recall that in our model, we suppose that our data, $\\mathbf{X}=\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$ is drawn from the mixture of $K$ number of Gaussian distributions. $X$ has D elements. For each observation $\\mathbf{x}_n$ we have a latent variable $\\mathbf{z}_n$ that is a 1-of-$K$ binary vector with elements $z_{nk}$. We denote the set of latent variable by $\\mathbf{Z}$. Recall that the distribution of $\\mathbf{Z}$ given the mixing coefficients, $\\pi$, is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{Z} | \\pi) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \n",
    "\\end{align}\n",
    "Recall also that the likelihood of the data is given by,\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) =\\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}\\left(\\mathbf{x}_n| \\mu_k, \\Sigma_k\\right)^{z_{nk}}\n",
    "\\end{align}\n",
    "Finally, in our basic model, we choose a Dirichlet prior for $\\pi$ \n",
    "\\begin{align}\n",
    "p(\\pi) = \\mathrm{Dir}(\\pi | \\alpha_0) = C(\\alpha_0) \\prod_{k=1}^K \\pi_k^{\\alpha_0 -1},\n",
    "\\end{align}\n",
    "where $C(\\alpha_0)$ is the normalizing constant for the Dirichlet distribution. We also choose a Normal-Inverse-Wishart prior for the mean and the covariance of the likelihood function\n",
    "\\begin{align}\n",
    "p(\\mu, \\Sigma) = p(\\mu | \\Sigma) p(\\Sigma) = \\prod_{k=1}^K \\mathcal{N}\\left(\\mu_k | \\mathbf{m}_0, \\mathbf{V}_0\\right) IW(\\Sigma_k|\\mathbf{S}_0, \\nu_0).\n",
    "\\end{align}\n",
    "Thus, the joint distribution of all the random variable is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Sigma) = p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) p(\\mathbf{Z} | \\pi) p(\\pi) p(\\mu | \\Sigma) p(\\Sigma)\n",
    "\\end{align}\n",
    "\n",
    "### Gibbs Sampler\n",
    "The full conditionals are as follows:\n",
    "1. $p(\\mathbf{z}_{n} = \\delta(k) | \\mathbf{x}_n, \\mu, \\Sigma, \\pi)  \\propto \\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)$.\n",
    "2. $p(\\pi|\\mathbf{Z}) = \\mathrm{Dir}(\\{ \\alpha_k + \\sum_{i=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))_{k=1}^K\\})$\n",
    "3. $p(\\mu_k | \\Sigma_k, \\mathbf{Z}, \\mathbf{X}) = \\mathcal{N}(\\mu_k | m_k, V_k)$\n",
    "4. $\\mathbf{V}_k^{-1} = \\mathbf{V}_0^{-1} + N_k\\Sigma_k^{-1}$\n",
    "5. $\\mathbf{m}_k = \\mathbf{V}_k(\\Sigma_k^{-1}N_k\\overline{\\mathbf{x}}_k + \\mathbf{V}_0^{-1}\\mathbf{m}_0)$\n",
    "6. $N_k = \\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))$\n",
    "7. $\\overline{\\mathbf{x}}_k = \\displaystyle \\frac{\\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))\\mathbf{x}_n}{N_k}$\n",
    "8. $p(\\Sigma_k | \\mu_k, \\mathbf{z}, \\mathbf{x}) = IW(\\Sigma_k | \\mathbf{S}_k, \\nu_k)$\n",
    "9. $\\mathbf{S}_k = \\mathbf{S}_0 + \\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))(\\mathbf{x}_n - \\mu_k)(\\mathbf{x}_n - \\mu_k)^\\top$\n",
    "10. $\\nu_k = \\nu_0 + N_k$\n",
    "\n",
    "The algorithm for the sampler is as follows:\n",
    "1. Instantiate the latent variables randomly.\n",
    "2. For $k=1...K$:\n",
    "    3. For $n=1...N$: update $z_i$ by sampling from $p(\\mathbf{z}_{n} = \\delta(k) | \\mathbf{x}_n, \\mu, \\Sigma, \\pi)$.\n",
    "    4. Update $\\pi$\n",
    "    5. Update variables for each component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEKCAYAAADdBdT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw5uISEMMbyIiDf0/QDi+OCxB5DgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4021198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "from sklearn import mixture\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "#-------------   data parameters\n",
    "K = 2 #number of components\n",
    "N = 400 #number of observations\n",
    "M = 2 #number of attributes per observation\n",
    "\n",
    "n_samples = int(N/2.0) #number of points in each component\n",
    "mu1 = np.array([10, 10]) #mean of component 1\n",
    "mu2 = np.array([6, 6]) #mean of component 1\n",
    "\n",
    "# generate random sample, two components\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate spherical data centered on mu1\n",
    "comp1 = np.random.randn(n_samples, M) + mu1\n",
    "\n",
    "# generate spherical data centered on mu2\n",
    "comp2 = np.random.randn(N - n_samples, M) + mu2\n",
    "\n",
    "# concatenate the two datasets into training set\n",
    "data = np.vstack([comp1, comp2])\n",
    "\n",
    "#plot the components\n",
    "plt.scatter(comp1[:, 0], comp1[:, 1], 4, color='r')\n",
    "plt.scatter(comp2[:, 0], comp2[:, 1], 4, color='b')\n",
    "\n",
    "plt.title('test data')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import invwishart, multivariate_normal, dirichlet\n",
    "\n",
    "# It would be easier to debug if we implement each update separately\n",
    "def update_Z(X, mu, Sigma, pi):\n",
    "    \"\"\"\n",
    "    X: NxD matrix\n",
    "    mu: Vector of k elements. Each element is a vector of D elements. \n",
    "    Sigma: Vector of k elements. Each element is a DxD covarianc ematrix\n",
    "    pi: Vector of k scalars\n",
    "    Returns Z, an NxK matrix made up of 1s and 0s (each row contains a single 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(X)\n",
    "    K = len(mu)\n",
    "    Z = np.zeros((N,K))\n",
    "\n",
    "    probs_vector = np.zeros(K)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(K):\n",
    "\n",
    "            probs_vector[j] = pi[j]*multivariate_normal.pdf(X[i], mean = mu[j], cov = Sigma[j])\n",
    "        normalization = np.sum(probs_vector)\n",
    "        probs_vector = probs_vector/normalization\n",
    "        Z[i] = np.random.multinomial(1, probs_vector)     \n",
    "    \n",
    "    return Z\n",
    "\n",
    "\n",
    "def update_N_k(Z):\n",
    "    \"\"\"Returns a k-element vector describing number of elements in each cluster\"\"\"\n",
    "    N_k = np.sum(Z, axis = 0)\n",
    "    \n",
    "    return N_k\n",
    "\n",
    "def update_pi(alpha_k, Z):\n",
    "    \"\"\"\n",
    "    alpha_k: Vector of K scalars\n",
    "    Z: an NxK matrix\n",
    "    pi: a vector of K scalars\n",
    "    \"\"\"\n",
    "    N_k = np.sum(Z, axis = 0)\n",
    "    pi = dirichlet.rvs(alpha_k+N_k)\n",
    "    return np.squeeze(pi)\n",
    "\n",
    "def update_mean_x_k(Z, X, N_k):\n",
    "    \"\"\"\n",
    "    Average value of observations belonging to cluster k\n",
    "    mean_x_k: Matrix of dimension KxD\n",
    "    \"\"\"\n",
    "    mean_x_k = np.zeros((len(N_k),len(X[0])))\n",
    "    for i in range(len(mean_x_k)):\n",
    "        for j in range(len(Z)):\n",
    "            mean_x_k[i]+=Z[j][i]*X[j]\n",
    "        mean_x_k[i] = mean_x_k[i]/N_k[i]\n",
    "\n",
    "    \n",
    "    return mean_x_k\n",
    "\n",
    "\n",
    "def update_nu_k(nu_0, N_k):\n",
    "    \"\"\"\n",
    "    returns a k-element vector\n",
    "    \"\"\"\n",
    "\n",
    "    nu_k = nu_0+N_k\n",
    "\n",
    "    return nu_k\n",
    "\n",
    "\n",
    "\n",
    "def update_V_k(V_0, N_k, Sigma_k):\n",
    "    \"\"\"\n",
    "    V_0: DxD matrix\n",
    "    N_k: Vector of k integers\n",
    "    Sigma_k: Vector of k elements, each element is a DxD covariance matrix\n",
    "    V_k: Vector of k elements, each element is a DxD covariance matrix\n",
    "    \"\"\"\n",
    "    V_k = np.zeros_like(Sigma_k)\n",
    "    V_0_inv = np.linalg.inv(V_0)\n",
    "    for i in range(len(V_k)):\n",
    "        inv = V_0_inv+N_k[i]*np.linalg.inv(Sigma_k[i])\n",
    "        V_k[i] = np.linalg.inv(inv)             \n",
    "        \n",
    "    return V_k\n",
    "    \n",
    "def update_m_k(V_k, Sigma_k, N_k, mean_x_k, V_0, m_0):\n",
    "    \"\"\"\n",
    "    V_0: DxD matrix\n",
    "    m_0: Vector of K elements, each element has D elements\n",
    "    N_k: Vector of k integers\n",
    "    Sigma_k: Vector of k elements, each element is a DxD covariance matrix\n",
    "    V_k: Vector of k elements, each element is a DxD covariance matrix\n",
    "    mean_x_k: DxK matrix describing average values of observations in each cluster\n",
    "    \"\"\"\n",
    "    V_0_inv = np.linalg.inv(V_0)\n",
    "    m_k = np.zeros((len(N_k), len(m_0)))\n",
    "    for i in range(len(m_k)):\n",
    "        Sig_inv = np.linalg.inv(Sigma_k[i])\n",
    "\n",
    "        m_k[i] = np.dot(V_k[i],(np.dot(Sig_inv,N_k[i]*mean_x_k[i])+np.dot(V_0_inv,m_0)))\n",
    "    return m_k\n",
    "\n",
    "def update_mu_k(m_k, V_k):\n",
    "    \"\"\"\n",
    "    m_k: Vector of k elements, each element has a vector with D elements\n",
    "    V_k: Vector of k elements, each element is a DxD covariance matrix\n",
    "    mu_k: Vector of k elements, each element has a vector with D elements\n",
    "    \"\"\"\n",
    "    mu_k = np.zeros_like(m_k)\n",
    "\n",
    "    for i in range(len(mu_k)):\n",
    "        mu_k[i] = multivariate_normal.rvs(mean = m_k[i], cov = V_k[i])\n",
    "\n",
    "    return mu_k\n",
    "\n",
    "def update_S_k(Z, X, mu_k, S_0):\n",
    "    \"\"\"\n",
    "    S_0: DxD matrix\n",
    "    Scale matrices for inverse wishart distribution\n",
    "    Returns array of k matrices (DxD)\n",
    "    \"\"\"\n",
    "    S_k = np.zeros((len(mu_k), S_0.shape[0], S_0.shape[1]))\n",
    "    for j in range(len(mu_k)):\n",
    "        S_k[j]=np.copy(S_0)\n",
    "        for i in range(len(Z)):\n",
    "            S_k[j]+=Z[i][j]*np.outer((X[i]-mu_k[j]),(X[i]-mu_k[j]))\n",
    "    return S_k\n",
    "\n",
    "def update_Sigma_k(S_k, nu_k):\n",
    "    \"\"\"\n",
    "    Covariance matrices for the k clusters\n",
    "    Returns array of k matrices of dimension DxD\n",
    "    \"\"\"\n",
    "    Sigma_k = np.zeros_like(S_k)\n",
    "    for i in range(len(nu_k)):\n",
    "        Sigma_k[i] = invwishart.rvs(nu_k[i], S_k[i])\n",
    "    return Sigma_k\n",
    "\n",
    "# this function can be reused even when we change the prior on pi \n",
    "def gibbs_gmm(K, X, pi, Z, mu, Sigma, nu_0, S_0, V_0, m_0, alpha_0):\n",
    "    \"\"\"\n",
    "    K: number of components\n",
    "    X: observations\n",
    "    nu_0: dof for inverse Wishart distribution\n",
    "    S_0: scale matrix for inverse Wishart distribution\n",
    "    m_0: Mean of normal-inverse-Wishart distribution\n",
    "    V_0: Covariance matrix of normal-inverse-Wishart distribution\n",
    "    alpha_0: parameter for Dirichlet distribution\n",
    "    \"\"\"\n",
    "    numsamples = 500\n",
    "\n",
    "    Z_samples = np.zeros((numsamples, Z.shape[0], Z.shape[1]))\n",
    "    mu_samples = np.zeros((numsamples, mu.shape[0], mu.shape[1]))\n",
    "    pi_samples = np.zeros((numsamples, K))\n",
    "    Sigma_samples = np.zeros((numsamples, K, Sigma.shape[1], Sigma.shape[2]))\n",
    "    nu_samples = np.zeros((numsamples, K))\n",
    "    V_samples = np.zeros((numsamples, K, V_0.shape[0],V_0.shape[1]))\n",
    "    m_samples = np.zeros((numsamples, K, m_0.shape[0]))\n",
    "    S_samples = np.zeros_like(Sigma_samples)\n",
    "\n",
    "    Z_samples[0] = Z\n",
    "    N_k = update_N_k(Z_samples[0])\n",
    "    pi_samples[0] = pi\n",
    "    mean_x_k = update_mean_x_k(Z_samples[0], X, N_k)\n",
    "    mu_samples[0] = mu\n",
    "    Sigma_samples[0] = Sigma\n",
    "    nu_samples[0] = update_nu_k(nu_0, N_k)\n",
    "    V_samples[0] = update_V_k(V_0, N_k, Sigma_samples[0])\n",
    "    m_samples[0] = update_m_k(V_samples[0], Sigma_samples[0], N_k, mean_x_k, V_0, m_0)\n",
    "    print m_samples[0]\n",
    "    S_samples[0] = update_S_k(Z_samples[0], X, mu_samples[0], S_0)\n",
    "\n",
    "    for i in range(1,numsamples):\n",
    "    \n",
    "        Z_samples[i]= update_Z(X, mu_samples[i-1], Sigma_samples[i-1], pi_samples[i-1])\n",
    "        N_k = update_N_k(Z_samples[i])\n",
    "        print i, N_k\n",
    "        #print \"+++\"\n",
    "        pi_samples[i] = update_pi(alpha_0, Z_samples[i])\n",
    "        mean_x_k = update_mean_x_k(Z_samples[i], X, N_k)\n",
    "        #if np.isnan(mean_x_k[0][0]) or np.isnan(mean_x_k[1][0]):\n",
    "        #    print pi_samples[i-1], mu_samples[i-1], Sigma_samples[i-1], N_k\n",
    "        mu_samples[i] = update_mu_k(m_samples[i-1], V_samples[i-1])\n",
    "        Sigma_samples[i] = update_Sigma_k(S_samples[i-1], nu_samples[i-1])\n",
    "        nu_samples[i] = update_nu_k(nu_0, N_k)\n",
    "        V_samples[i]= update_V_k(V_0, N_k, Sigma_samples[i])\n",
    "        m_samples[i] = update_m_k(V_samples[i], Sigma_samples[i], N_k, mean_x_k, V_0, m_0)\n",
    "        S_samples[i] = update_S_k(Z_samples[i], X, mu_samples[i], S_0)\n",
    "\n",
    "    return Z_samples, pi_samples, mu_samples, Sigma_samples, nu_samples, V_samples, m_samples, S_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.94290529  10.02034371]\n",
      " [  5.93828041   5.92667786]]\n",
      "1 [ 198.  202.]\n",
      "2 [ 204.  196.]\n",
      "3 [ 200.  200.]\n",
      "4 [ 201.  199.]\n",
      "5 [ 201.  199.]\n",
      "6 [ 200.  200.]\n",
      "7 [ 200.  200.]\n",
      "8 [ 200.  200.]\n",
      "9 [ 200.  200.]\n",
      "10 [ 199.  201.]\n",
      "11 [ 200.  200.]\n",
      "12 [ 201.  199.]\n",
      "13 [ 200.  200.]\n",
      "14 [ 200.  200.]\n",
      "15 [ 200.  200.]\n",
      "16 [ 200.  200.]\n",
      "17 [ 200.  200.]\n",
      "18 [ 200.  200.]\n",
      "19 [ 200.  200.]\n",
      "20 [ 200.  200.]\n",
      "21 [ 200.  200.]\n",
      "22 [ 201.  199.]\n",
      "23 [ 201.  199.]\n",
      "24 [ 200.  200.]\n",
      "25 [ 199.  201.]\n",
      "26 [ 200.  200.]\n",
      "27 [ 200.  200.]\n",
      "28 [ 200.  200.]\n",
      "29 [ 200.  200.]\n",
      "30 [ 200.  200.]\n",
      "31 [ 200.  200.]\n",
      "32 [ 200.  200.]\n",
      "33 [ 200.  200.]\n",
      "34 [ 199.  201.]\n",
      "35 [ 200.  200.]\n",
      "36 [ 200.  200.]\n",
      "37 [ 200.  200.]\n",
      "38 [ 200.  200.]\n",
      "39 [ 200.  200.]\n",
      "40 [ 200.  200.]\n",
      "41 [ 200.  200.]\n",
      "42 [ 200.  200.]\n",
      "43 [ 200.  200.]\n",
      "44 [ 200.  200.]\n",
      "45 [ 200.  200.]\n",
      "46 [ 200.  200.]\n",
      "47 [ 200.  200.]\n",
      "48 [ 200.  200.]\n",
      "49 [ 200.  200.]\n",
      "50 [ 200.  200.]\n",
      "51 [ 201.  199.]\n",
      "52 [ 200.  200.]\n",
      "53 [ 200.  200.]\n",
      "54 [ 200.  200.]\n",
      "55 [ 200.  200.]\n",
      "56 [ 200.  200.]\n",
      "57 [ 200.  200.]\n",
      "58 [ 200.  200.]\n",
      "59 [ 200.  200.]\n",
      "60 [ 200.  200.]\n",
      "61 [ 201.  199.]\n",
      "62 [ 200.  200.]\n",
      "63 [ 201.  199.]\n",
      "64 [ 200.  200.]\n",
      "65 [ 200.  200.]\n",
      "66 [ 200.  200.]\n",
      "67 [ 200.  200.]\n",
      "68 [ 200.  200.]\n",
      "69 [ 201.  199.]\n",
      "70 [ 200.  200.]\n",
      "71 [ 200.  200.]\n",
      "72 [ 200.  200.]\n",
      "73 [ 200.  200.]\n",
      "74 [ 200.  200.]\n",
      "75 [ 200.  200.]\n",
      "76 [ 200.  200.]\n",
      "77 [ 200.  200.]\n",
      "78 [ 200.  200.]\n",
      "79 [ 200.  200.]\n",
      "80 [ 200.  200.]\n",
      "81 [ 200.  200.]\n",
      "82 [ 200.  200.]\n",
      "83 [ 200.  200.]\n",
      "84 [ 200.  200.]\n",
      "85 [ 200.  200.]\n",
      "86 [ 200.  200.]\n",
      "87 [ 200.  200.]\n",
      "88 [ 200.  200.]\n",
      "89 [ 200.  200.]\n",
      "90 [ 200.  200.]\n",
      "91 [ 201.  199.]\n",
      "92 [ 200.  200.]\n",
      "93 [ 201.  199.]\n",
      "94 [ 200.  200.]\n",
      "95 [ 199.  201.]\n",
      "96 [ 200.  200.]\n",
      "97 [ 200.  200.]\n",
      "98 [ 201.  199.]\n",
      "99 [ 200.  200.]\n",
      "100 [ 200.  200.]\n",
      "101 [ 200.  200.]\n",
      "102 [ 200.  200.]\n",
      "103 [ 200.  200.]\n",
      "104 [ 200.  200.]\n",
      "105 [ 200.  200.]\n",
      "106 [ 200.  200.]\n",
      "107 [ 200.  200.]\n",
      "108 [ 201.  199.]\n",
      "109 [ 200.  200.]\n",
      "110 [ 199.  201.]\n",
      "111 [ 200.  200.]\n",
      "112 [ 201.  199.]\n",
      "113 [ 201.  199.]\n",
      "114 [ 200.  200.]\n",
      "115 [ 200.  200.]\n",
      "116 [ 199.  201.]\n",
      "117 [ 200.  200.]\n",
      "118 [ 200.  200.]\n",
      "119 [ 200.  200.]\n",
      "120 [ 200.  200.]\n",
      "121 [ 200.  200.]\n",
      "122 [ 200.  200.]\n",
      "123 [ 200.  200.]\n",
      "124 [ 200.  200.]\n",
      "125 [ 200.  200.]\n",
      "126 [ 200.  200.]\n",
      "127 [ 200.  200.]\n",
      "128 [ 200.  200.]\n",
      "129 [ 200.  200.]\n",
      "130 [ 200.  200.]\n",
      "131 [ 200.  200.]\n",
      "132 [ 200.  200.]\n",
      "133 [ 199.  201.]\n",
      "134 [ 200.  200.]\n",
      "135 [ 200.  200.]\n",
      "136 [ 200.  200.]\n",
      "137 [ 200.  200.]\n",
      "138 [ 200.  200.]\n",
      "139 [ 199.  201.]\n",
      "140 [ 201.  199.]\n",
      "141 [ 200.  200.]\n",
      "142 [ 199.  201.]\n",
      "143 [ 200.  200.]\n",
      "144 [ 200.  200.]\n",
      "145 [ 200.  200.]\n",
      "146 [ 200.  200.]\n",
      "147 [ 200.  200.]\n",
      "148 [ 200.  200.]\n",
      "149 [ 200.  200.]\n",
      "150 [ 200.  200.]\n",
      "151 [ 200.  200.]\n",
      "152 [ 200.  200.]\n",
      "153 [ 200.  200.]\n",
      "154 [ 201.  199.]\n",
      "155 [ 200.  200.]\n",
      "156 [ 200.  200.]\n",
      "157 [ 201.  199.]\n",
      "158 [ 200.  200.]\n",
      "159 [ 200.  200.]\n",
      "160 [ 200.  200.]\n",
      "161 [ 200.  200.]\n",
      "162 [ 200.  200.]\n",
      "163 [ 200.  200.]\n",
      "164 [ 200.  200.]\n",
      "165 [ 200.  200.]\n",
      "166 [ 200.  200.]\n",
      "167 [ 200.  200.]\n",
      "168 [ 200.  200.]\n",
      "169 [ 200.  200.]\n",
      "170 [ 199.  201.]\n",
      "171 [ 200.  200.]\n",
      "172 [ 201.  199.]\n",
      "173 [ 200.  200.]\n",
      "174 [ 199.  201.]\n",
      "175 [ 200.  200.]\n",
      "176 [ 200.  200.]\n",
      "177 [ 199.  201.]\n",
      "178 [ 200.  200.]\n",
      "179 [ 200.  200.]\n",
      "180 [ 200.  200.]\n",
      "181 [ 201.  199.]\n",
      "182 [ 200.  200.]\n",
      "183 [ 200.  200.]\n",
      "184 [ 200.  200.]\n",
      "185 [ 199.  201.]\n",
      "186 [ 201.  199.]\n",
      "187 [ 199.  201.]\n",
      "188 [ 200.  200.]\n",
      "189 [ 200.  200.]\n",
      "190 [ 201.  199.]\n",
      "191 [ 200.  200.]\n",
      "192 [ 200.  200.]\n",
      "193 [ 200.  200.]\n",
      "194 [ 200.  200.]\n",
      "195 [ 200.  200.]\n",
      "196 [ 199.  201.]\n",
      "197 [ 200.  200.]\n",
      "198 [ 199.  201.]\n",
      "199 [ 199.  201.]\n",
      "200 [ 200.  200.]\n",
      "201 [ 199.  201.]\n",
      "202 [ 200.  200.]\n",
      "203 [ 198.  202.]\n",
      "204 [ 200.  200.]\n",
      "205 [ 199.  201.]\n",
      "206 [ 200.  200.]\n",
      "207 [ 201.  199.]\n",
      "208 [ 200.  200.]\n",
      "209 [ 200.  200.]\n",
      "210 [ 199.  201.]\n",
      "211 [ 199.  201.]\n",
      "212 [ 200.  200.]\n",
      "213 [ 199.  201.]\n",
      "214 [ 200.  200.]\n",
      "215 [ 200.  200.]\n",
      "216 [ 199.  201.]\n",
      "217 [ 200.  200.]\n",
      "218 [ 200.  200.]\n",
      "219 [ 200.  200.]\n",
      "220 [ 200.  200.]\n",
      "221 [ 200.  200.]\n",
      "222 [ 199.  201.]\n",
      "223 [ 200.  200.]\n",
      "224 [ 200.  200.]\n",
      "225 [ 202.  198.]\n",
      "226 [ 200.  200.]\n",
      "227 [ 199.  201.]\n",
      "228 [ 200.  200.]\n",
      "229 [ 200.  200.]\n",
      "230 [ 200.  200.]\n",
      "231 [ 200.  200.]\n",
      "232 [ 201.  199.]\n",
      "233 [ 200.  200.]\n",
      "234 [ 200.  200.]\n",
      "235 [ 200.  200.]\n",
      "236 [ 201.  199.]\n",
      "237 [ 200.  200.]\n",
      "238 [ 201.  199.]\n",
      "239 [ 201.  199.]\n",
      "240 [ 200.  200.]\n",
      "241 [ 200.  200.]\n",
      "242 [ 200.  200.]\n",
      "243 [ 200.  200.]\n",
      "244 [ 201.  199.]\n",
      "245 [ 200.  200.]\n",
      "246 [ 200.  200.]\n",
      "247 [ 200.  200.]\n",
      "248 [ 199.  201.]\n",
      "249 [ 200.  200.]\n",
      "250 [ 200.  200.]\n",
      "251 [ 201.  199.]\n",
      "252 [ 201.  199.]\n",
      "253 [ 200.  200.]\n",
      "254 [ 200.  200.]\n",
      "255 [ 200.  200.]\n",
      "256 [ 200.  200.]\n",
      "257 [ 200.  200.]\n",
      "258 [ 200.  200.]\n",
      "259 [ 200.  200.]\n",
      "260 [ 200.  200.]\n",
      "261 [ 200.  200.]\n",
      "262 [ 200.  200.]\n",
      "263 [ 200.  200.]\n",
      "264 [ 200.  200.]\n",
      "265 [ 199.  201.]\n",
      "266 [ 201.  199.]\n",
      "267 [ 200.  200.]\n",
      "268 [ 199.  201.]\n",
      "269 [ 200.  200.]\n",
      "270 [ 200.  200.]\n",
      "271 [ 200.  200.]\n",
      "272 [ 200.  200.]\n",
      "273 [ 200.  200.]\n",
      "274 [ 198.  202.]\n",
      "275 [ 200.  200.]\n",
      "276 [ 199.  201.]\n",
      "277 [ 200.  200.]\n",
      "278 [ 200.  200.]\n",
      "279 [ 199.  201.]\n",
      "280 [ 200.  200.]\n",
      "281 [ 200.  200.]\n",
      "282 [ 200.  200.]\n",
      "283 [ 199.  201.]\n",
      "284 [ 200.  200.]\n",
      "285 [ 199.  201.]\n",
      "286 [ 200.  200.]\n",
      "287 [ 199.  201.]\n",
      "288 [ 200.  200.]\n",
      "289 [ 200.  200.]\n",
      "290 [ 200.  200.]\n",
      "291 [ 200.  200.]\n",
      "292 [ 200.  200.]\n",
      "293 [ 201.  199.]\n",
      "294 [ 201.  199.]\n",
      "295 [ 200.  200.]\n",
      "296 [ 200.  200.]\n",
      "297 [ 200.  200.]\n",
      "298 [ 200.  200.]\n",
      "299 [ 200.  200.]\n",
      "300 [ 200.  200.]\n",
      "301 [ 200.  200.]\n",
      "302 [ 200.  200.]\n",
      "303 [ 200.  200.]\n",
      "304 [ 200.  200.]\n",
      "305 [ 201.  199.]\n",
      "306 [ 200.  200.]\n",
      "307 [ 200.  200.]\n",
      "308 [ 200.  200.]\n",
      "309 [ 200.  200.]\n",
      "310 [ 200.  200.]\n",
      "311 [ 200.  200.]\n",
      "312 [ 200.  200.]\n",
      "313 [ 199.  201.]\n",
      "314 [ 200.  200.]\n",
      "315 [ 200.  200.]\n",
      "316 [ 200.  200.]\n",
      "317 [ 200.  200.]\n",
      "318 [ 200.  200.]\n",
      "319 [ 199.  201.]\n",
      "320 [ 200.  200.]\n",
      "321 [ 200.  200.]\n",
      "322 [ 200.  200.]\n",
      "323 [ 200.  200.]\n",
      "324 [ 199.  201.]\n",
      "325 [ 199.  201.]\n",
      "326 [ 200.  200.]\n",
      "327 [ 200.  200.]\n",
      "328 [ 200.  200.]\n",
      "329 [ 200.  200.]\n",
      "330 [ 200.  200.]\n",
      "331 [ 200.  200.]\n",
      "332 [ 200.  200.]\n",
      "333 [ 200.  200.]\n",
      "334 [ 200.  200.]\n",
      "335 [ 200.  200.]\n",
      "336 [ 200.  200.]\n",
      "337 [ 200.  200.]\n",
      "338 [ 200.  200.]\n",
      "339 [ 200.  200.]\n",
      "340 [ 200.  200.]\n",
      "341 [ 200.  200.]\n",
      "342 [ 201.  199.]\n",
      "343 [ 200.  200.]\n",
      "344 [ 200.  200.]\n",
      "345 [ 200.  200.]\n",
      "346 [ 200.  200.]\n",
      "347 [ 200.  200.]\n",
      "348 [ 200.  200.]\n",
      "349 [ 200.  200.]\n",
      "350 [ 200.  200.]\n",
      "351 [ 200.  200.]\n",
      "352 [ 201.  199.]\n",
      "353 [ 200.  200.]\n",
      "354 [ 200.  200.]\n",
      "355 [ 200.  200.]\n",
      "356 [ 201.  199.]\n",
      "357 [ 200.  200.]\n",
      "358 [ 200.  200.]\n",
      "359 [ 200.  200.]\n",
      "360 [ 199.  201.]\n",
      "361 [ 200.  200.]\n",
      "362 [ 200.  200.]\n",
      "363 [ 200.  200.]\n",
      "364 [ 200.  200.]\n",
      "365 [ 199.  201.]\n",
      "366 [ 200.  200.]\n",
      "367 [ 200.  200.]\n",
      "368 [ 200.  200.]\n",
      "369 [ 200.  200.]\n",
      "370 [ 200.  200.]\n",
      "371 [ 200.  200.]\n",
      "372 [ 200.  200.]\n",
      "373 [ 200.  200.]\n",
      "374 [ 200.  200.]\n",
      "375 [ 199.  201.]\n",
      "376 [ 200.  200.]\n",
      "377 [ 201.  199.]\n",
      "378 [ 200.  200.]\n",
      "379 [ 200.  200.]\n",
      "380 [ 200.  200.]\n",
      "381 [ 201.  199.]\n",
      "382 [ 200.  200.]\n",
      "383 [ 201.  199.]\n",
      "384 [ 201.  199.]\n",
      "385 [ 200.  200.]\n",
      "386 [ 200.  200.]\n",
      "387 [ 200.  200.]\n",
      "388 [ 200.  200.]\n",
      "389 [ 200.  200.]\n",
      "390 [ 200.  200.]\n",
      "391 [ 200.  200.]\n",
      "392 [ 200.  200.]\n",
      "393 [ 200.  200.]\n",
      "394 [ 200.  200.]\n",
      "395 [ 201.  199.]\n",
      "396 [ 200.  200.]\n",
      "397 [ 200.  200.]\n",
      "398 [ 199.  201.]\n",
      "399 [ 201.  199.]\n",
      "400 [ 200.  200.]\n",
      "401 [ 200.  200.]\n",
      "402 [ 200.  200.]\n",
      "403 [ 200.  200.]\n",
      "404 [ 201.  199.]\n",
      "405 [ 200.  200.]\n",
      "406 [ 200.  200.]\n",
      "407 [ 200.  200.]\n",
      "408 [ 200.  200.]\n",
      "409 [ 200.  200.]\n",
      "410 [ 200.  200.]\n",
      "411 [ 200.  200.]\n",
      "412 [ 200.  200.]\n",
      "413 [ 200.  200.]\n",
      "414 [ 200.  200.]\n",
      "415 [ 200.  200.]\n",
      "416 [ 200.  200.]\n",
      "417 [ 200.  200.]\n",
      "418 [ 200.  200.]\n",
      "419 [ 200.  200.]\n",
      "420 [ 201.  199.]\n",
      "421 [ 200.  200.]\n",
      "422 [ 200.  200.]\n",
      "423 [ 199.  201.]\n",
      "424 [ 200.  200.]\n",
      "425 [ 200.  200.]\n",
      "426 [ 200.  200.]\n",
      "427 [ 200.  200.]\n",
      "428 [ 201.  199.]\n",
      "429 [ 201.  199.]\n",
      "430 [ 200.  200.]\n",
      "431 [ 200.  200.]\n",
      "432 [ 200.  200.]\n",
      "433 [ 200.  200.]\n",
      "434 [ 199.  201.]\n",
      "435 [ 201.  199.]\n",
      "436 [ 200.  200.]\n",
      "437 [ 200.  200.]\n",
      "438 [ 200.  200.]\n",
      "439 [ 200.  200.]\n",
      "440 [ 200.  200.]\n",
      "441 [ 200.  200.]\n",
      "442 [ 200.  200.]\n",
      "443 [ 200.  200.]\n",
      "444 [ 200.  200.]\n",
      "445 [ 201.  199.]\n",
      "446 [ 200.  200.]\n",
      "447 [ 200.  200.]\n",
      "448 [ 200.  200.]\n",
      "449 [ 200.  200.]\n",
      "450 [ 201.  199.]\n",
      "451 [ 200.  200.]\n",
      "452 [ 200.  200.]\n",
      "453 [ 200.  200.]\n",
      "454 [ 199.  201.]\n",
      "455 [ 200.  200.]\n",
      "456 [ 200.  200.]\n",
      "457 [ 200.  200.]\n",
      "458 [ 200.  200.]\n",
      "459 [ 200.  200.]\n",
      "460 [ 200.  200.]\n",
      "461 [ 200.  200.]\n",
      "462 [ 200.  200.]\n",
      "463 [ 200.  200.]\n",
      "464 [ 200.  200.]\n",
      "465 [ 200.  200.]\n",
      "466 [ 200.  200.]\n",
      "467 [ 200.  200.]\n",
      "468 [ 199.  201.]\n",
      "469 [ 200.  200.]\n",
      "470 [ 200.  200.]\n",
      "471 [ 199.  201.]\n",
      "472 [ 200.  200.]\n",
      "473 [ 199.  201.]\n",
      "474 [ 200.  200.]\n",
      "475 [ 200.  200.]\n",
      "476 [ 200.  200.]\n",
      "477 [ 200.  200.]\n",
      "478 [ 200.  200.]\n",
      "479 [ 200.  200.]\n",
      "480 [ 200.  200.]\n",
      "481 [ 199.  201.]\n",
      "482 [ 199.  201.]\n",
      "483 [ 200.  200.]\n",
      "484 [ 200.  200.]\n",
      "485 [ 201.  199.]\n",
      "486 [ 200.  200.]\n",
      "487 [ 200.  200.]\n",
      "488 [ 200.  200.]\n",
      "489 [ 201.  199.]\n",
      "490 [ 200.  200.]\n",
      "491 [ 200.  200.]\n",
      "492 [ 200.  200.]\n",
      "493 [ 200.  200.]\n",
      "494 [ 200.  200.]\n",
      "495 [ 201.  199.]\n",
      "496 [ 200.  200.]\n",
      "497 [ 200.  200.]\n",
      "498 [ 200.  200.]\n",
      "499 [ 200.  200.]\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "K = 2\n",
    "X = data\n",
    "pi = np.array([0.44, 0.55])\n",
    "\n",
    "Sigma = np.array([np.eye(2), np.eye(2)])\n",
    "\n",
    "mu = np.array([[11.2, 10.7],[5.3, 6.5]]) \n",
    "Z  = update_Z(data, mu, Sigma, pi)\n",
    "\n",
    "nu_0 = 2\n",
    "S_0 = np.eye(2)\n",
    "V_0 =  np.eye(2)\n",
    "m_0 = np.array([8., 8.])  \n",
    "alpha_0 = np.array([1., 1.])\n",
    "\n",
    "\n",
    "\n",
    "Z_samples, pi_samples, mu_samples, Sigma_samples, nu_samples, V_samples, m_samples, S_samples= gibbs_gmm(K, \n",
    "                                                                                                         X, \n",
    "                                                                                                         pi, \n",
    "                                                                                                         Z, \n",
    "                                                                                                         mu, \n",
    "                                                                                                         Sigma, \n",
    "                                                                                                         nu_0, \n",
    "                                                                                                         S_0, \n",
    "                                                                                                         V_0, \n",
    "                                                                                                         m_0, \n",
    "                                                                                                         alpha_0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.91711958  10.0053262 ]\n",
      " [  5.89819451   5.88759841]] [[ 0.08992357  0.07634173]\n",
      " [ 0.07223782  0.07959332]]\n"
     ]
    }
   ],
   "source": [
    "def get_mu_posterior(mu_samples, K):\n",
    "    mu_avg = np.zeros_like(mu_samples[0])\n",
    "    mu_std = np.zeros_like(mu_samples[0])\n",
    "    for i in range(K):\n",
    "        mu_avg[i] = np.mean(mu_samples[:,i], axis = 0)\n",
    "        mu_std[i] = np.std(mu_samples[:,i], axis = 0)\n",
    "    return mu_avg, mu_std\n",
    "\n",
    "mu_avg, mu_std = get_mu_posterior(mu_samples, 2)\n",
    "print mu_avg, mu_std\n",
    "\n",
    "#print mu_samples[:21]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.89500293  7.26369482]\n"
     ]
    }
   ],
   "source": [
    "mus = samples[:,2]\n",
    "running = np.zeros(2)\n",
    "for i in mus:\n",
    "    running +=i[1]\n",
    "    \n",
    "print running/500.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.25337123,  12.18898369])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal.rvs(mean = mu[0], cov=np.eye(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.55648316  12.25298037]\n",
      " [  0.           0.        ]]\n",
      "[[ 10.55648316  12.25298037]\n",
      " [  6.39750687   6.92003442]]\n"
     ]
    }
   ],
   "source": [
    "mu_k = np.zeros_like((mu))\n",
    "for i in range(len(mu_k)):\n",
    "    mu_k[i] = multivariate_normal.rvs(mean = mu[i], cov = np.eye(2))\n",
    "    print mu_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.2  11.2]\n",
      " [  5.4   6.3]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 10.12187158  12.78942693]\n",
      " [  0.           0.        ]]\n",
      "[[ 10.12187158  12.78942693]\n",
      " [  6.6314547    5.21452637]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 10.12187158,  12.78942693],\n",
       "       [  6.6314547 ,   5.21452637]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_mu_k(np.array([[9.2, 11.2],[5.4, 6.3]]) , np.array([np.eye(2), np.eye(2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 48.  52.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.17925766,  9.15989398],\n",
       "       [ 6.90961277,  7.20390299]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z  = update_Z(data, mu, Sigma, pi)\n",
    "N_k = update_N_k(Z)\n",
    "print N_k\n",
    "update_mean_x_k(Z, data, N_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2L, 2L, 2L)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def update_Sigma_k(S_k, nu_k):\n",
    "    \"\"\"\n",
    "    Covariance matrices for the k clusters\n",
    "    Returns array of k matrices of dimension DxD\n",
    "    \"\"\"\n",
    "    Sigma_k = np.empty(len(nu_k), dtype = object)\n",
    "    for i in range(len(nu_k)):\n",
    "        Sigma_k[i] = invwishart.rvs(nu_k[i], S_k[i])\n",
    "    return Sigma_k\n",
    "\n",
    "Sigma = np.array([np.eye(2), np.eye(2)])\n",
    "Sigma.shape\n",
    "np.zeros_like(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
