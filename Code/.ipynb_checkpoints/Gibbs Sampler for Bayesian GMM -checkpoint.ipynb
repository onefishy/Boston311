{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampler for Bayesian GMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Target Distribution\n",
    "Recall that in our model, we suppose that our data, $\\mathbf{X}=\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_K\\}$ is drawn from the mixture of $K$ number of Gaussian distributions. For each observation $\\mathbf{x}_n$ we have a latent variable $\\mathbf{z}_n$ that is a 1-of-$K$ binary vector with elements $z_{nk}$. We denote the set of latent variable by $\\mathbf{Z}$. Recall that the distibution of $\\mathbf{Z}$ given the mixing coefficients, $\\pi$, is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{Z} | \\pi) = \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}} \n",
    "\\end{align}\n",
    "Recall also that the likelihood of the data is given by,\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) =\\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}\\left(\\mathbf{x}_n| \\mu_k, \\Sigma_k\\right)^{z_{nk}}\n",
    "\\end{align}\n",
    "Finally, in our basic model, we choose a Dirichlet prior for $\\pi$ \n",
    "\\begin{align}\n",
    "p(\\pi) = \\mathrm{Dir}(\\pi | \\alpha_0) = C(\\alpha_0) \\prod_{k=1}^K \\pi_k^{\\alpha_0 -1},\n",
    "\\end{align}\n",
    "where $C(\\alpha_0)$ is the normalizing constant for the Dirichlet distribution. We also choose a Normal-Inverse-Wishart prior for the mean and the covariance of the likelihood function\n",
    "\\begin{align}\n",
    "p(\\mu, \\Sigma) = p(\\mu | \\Sigma) p(\\Sigma) = \\prod_{k=1}^K \\mathcal{N}\\left(\\mu_k | \\mathbf{m}_0, \\mathbf{V}_0\\right) IW(\\Sigma_k|\\mathbf{S}_0, \\nu_0).\n",
    "\\end{align}\n",
    "Thus, the joint distribution of all the random variable is given by\n",
    "\\begin{align}\n",
    "p(\\mathbf{X}, \\mathbf{Z}, \\pi, \\mu, \\Sigma) = p(\\mathbf{X} | \\mathbf{Z}, \\mu, \\Sigma) p(\\mathbf{Z} | \\pi) p(\\pi) p(\\mu | \\Sigma) p(\\Sigma)\n",
    "\\end{align}\n",
    "\n",
    "### Gibbs Samper\n",
    "The full conditionals are as follows:\n",
    "1. $p(\\mathbf{z}_{n} = \\delta(k) | \\mathbf{x}_n, \\mu, \\Sigma, \\pi)  \\propto \\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)$.\n",
    "2. $p(\\pi|\\mathbf{Z}) = \\mathrm{Dir}(\\{ \\alpha_k + \\sum_{i=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))_{k=1}^K\\})$\n",
    "3. $p(\\mu_k | \\Sigma_k, \\mathbf{Z}, \\mathbf{X}) = \\mathcal{N}(\\mu_k | m_k, V_k)$\n",
    "4. $\\mathbf{V}_k^{-1} = \\mathbf{V}_0^{-1} + N_k\\Sigma_k^{-1}$\n",
    "5. $\\mathbf{m}_k = \\mathbf{V}_k(\\Sigma_k^{-1}N_k\\overline{\\mathbf{x}}_k + \\mathbf{V}_0^{-1}\\mathbf{m}_0)$\n",
    "6. $N_k = \\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))$\n",
    "7. $\\overline{\\mathbf{x}}_k = \\displaystyle \\frac{\\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))\\mathbf{x}_n}{N_k}$\n",
    "8. $p(\\Sigma_k | \\mu_k, \\mathbf{z}, \\mathbf{x}) = IW(\\Sigma_k | \\mathbf{S}_k, \\nu_k)$\n",
    "9. $\\mathbf{S}_k = \\mathbf{S}_0 + \\sum_{n=1}^N \\mathbb{I}(\\mathbf{z}_{n} = \\delta(k))(\\mathbf{x}_n - \\mu_k)(\\mathbf{x}_n - \\mu_k)^\\top$\n",
    "10. $\\nu_k = \\nu_0 + N_k$\n",
    "\n",
    "The algorithm for the sampler is as follows:\n",
    "1. Instantiate the latent variables randomly.\n",
    "2. For $k=1...K$:\n",
    "    3. For $n=1...N$: update $z_i$ by sampling from $p(\\mathbf{z}_{n} = \\delta(k) | \\mathbf{x}_n, \\mu, \\Sigma, \\pi)$.\n",
    "    4. Update $\\pi$\n",
    "    5. Update variables for each component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It would be easier to debug if we implement each update separately\n",
    "def update_Z(delta_k, X, mu, Sigma, pi):\n",
    "    return Z\n",
    "\n",
    "def update_pi(alpha_k, Z):\n",
    "    return pi\n",
    "\n",
    "def update_mu_k(m_k, V_k):\n",
    "    return mu_k\n",
    "\n",
    "def update_V_k(N_k, Sigma_k):\n",
    "    return V_k\n",
    "\n",
    "def update_m_k(V_k, Sigma_k, N_k, mean_x_k, V_0, m_0):\n",
    "    return m_k\n",
    "\n",
    "def update_N_k(Z):\n",
    "    return N_k\n",
    "\n",
    "def update_mean_x_k(Z, X, N_k):\n",
    "    return mean_x_k\n",
    "\n",
    "def update_Sigma_k(S_k, nu_k):\n",
    "    return Sigma_k\n",
    "\n",
    "def update_S_k(S_0, Z, X, mu_k, S_0):\n",
    "    return S_k\n",
    "\n",
    "def update_nu_k(nu_0, N_k):\n",
    "    return nu_k\n",
    "\n",
    "# this function can be reused even when we change the prior on pi \n",
    "def gibbs_gmm(K, X, nu_0, S_0, V_0, m_0, alpha_0):\n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
